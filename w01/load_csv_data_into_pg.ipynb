{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4cb2dfd-52c2-41ca-bb3c-0cdf9d431d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from time import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b18b710-4ca5-4220-adea-7745a6195c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = (Path().resolve().parent / \"data/csv/yellow/\")\n",
    "DATA_FILES = list(DATA_DIR.walk())[0][2]\n",
    "DATA_CHUNK_SIZE = 100_000\n",
    "DEST_TABLE = \"yellow_taxi_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5a01010-360e-4184-a509-e364cec10dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yellow_tripdata_2019-01.csv'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74cd22ad-717e-4804-990d-590dd4be303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования CamelCase в snake_case\n",
    "def camel_to_snake(name):\n",
    "    return re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower()\n",
    "\n",
    "# Функция проверяет существует ли в БД указанная таблица \n",
    "def db_table_exists(table_name: str, engine: Engine) -> bool:\n",
    "    inspector = inspect(engine)\n",
    "    return inspector.has_table(table_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317de4c-fb46-4b67-863a-78e228878734",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://root:root@localhost:5432/ny_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a4aebff-4c30-4550-9816-486aa40b1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = DATA_DIR / DATA_FILES[0]\n",
    "df = pd.read_csv(fn, nrows=10, parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef07e1b4-87dc-43e6-a8a3-8c6bb89b4f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   VendorID               10 non-null     int64         \n",
      " 1   tpep_pickup_datetime   10 non-null     datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  10 non-null     datetime64[ns]\n",
      " 3   passenger_count        10 non-null     int64         \n",
      " 4   trip_distance          10 non-null     float64       \n",
      " 5   RatecodeID             10 non-null     int64         \n",
      " 6   store_and_fwd_flag     10 non-null     object        \n",
      " 7   PULocationID           10 non-null     int64         \n",
      " 8   DOLocationID           10 non-null     int64         \n",
      " 9   payment_type           10 non-null     int64         \n",
      " 10  fare_amount            10 non-null     float64       \n",
      " 11  extra                  10 non-null     float64       \n",
      " 12  mta_tax                10 non-null     float64       \n",
      " 13  tip_amount             10 non-null     float64       \n",
      " 14  tolls_amount           10 non-null     float64       \n",
      " 15  improvement_surcharge  10 non-null     float64       \n",
      " 16  total_amount           10 non-null     float64       \n",
      " 17  congestion_surcharge   0 non-null      float64       \n",
      "dtypes: datetime64[ns](2), float64(9), int64(6), object(1)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36fe6fd5-99b8-442b-b2b1-ff45ff345590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10 entries, 0 to 9\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   vendor_id              10 non-null     int64         \n",
      " 1   tpep_pickup_datetime   10 non-null     datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  10 non-null     datetime64[ns]\n",
      " 3   passenger_count        10 non-null     int64         \n",
      " 4   trip_distance          10 non-null     float64       \n",
      " 5   ratecode_id            10 non-null     int64         \n",
      " 6   store_and_fwd_flag     10 non-null     object        \n",
      " 7   pu_location_id         10 non-null     int64         \n",
      " 8   do_location_id         10 non-null     int64         \n",
      " 9   payment_type           10 non-null     int64         \n",
      " 10  fare_amount            10 non-null     float64       \n",
      " 11  extra                  10 non-null     float64       \n",
      " 12  mta_tax                10 non-null     float64       \n",
      " 13  tip_amount             10 non-null     float64       \n",
      " 14  tolls_amount           10 non-null     float64       \n",
      " 15  improvement_surcharge  10 non-null     float64       \n",
      " 16  total_amount           10 non-null     float64       \n",
      " 17  congestion_surcharge   0 non-null      float64       \n",
      "dtypes: datetime64[ns](2), float64(9), int64(6), object(1)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "new_columns_name = {\n",
    "    'VendorID': 'vendor_id',\n",
    "    'tpep_pickup_datetime': 'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'tpep_dropoff_datetime',\n",
    "    'passenger_count': 'passenger_count',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'RatecodeID': 'ratecode_id',\n",
    "    'store_and_fwd_flag': 'store_and_fwd_flag',\n",
    "    'PULocationID': 'pu_location_id',\n",
    "    'DOLocationID': 'do_location_id',\n",
    "    'payment_type': 'payment_type',\n",
    "    'fare_amount': 'fare_amount',\n",
    "    'extra': 'extra',\n",
    "    'mta_tax': 'mta_tax',\n",
    "    'tip_amount': 'tip_amount',\n",
    "    'tolls_amount': 'tolls_amount',\n",
    "    'improvement_surcharge': 'improvement_surcharge',\n",
    "    'total_amount': 'total_amount',\n",
    "    'congestion_surcharge': 'congestion_surcharge'\n",
    "}\n",
    "df = df.rename(columns=new_columns_name)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73f0f935-23c5-4660-84e7-2f595180c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\tvendor_id BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\tratecode_id BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\tpu_location_id BIGINT, \n",
      "\tdo_location_id BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name=DEST_TABLE, con=engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56d22446-7f16-440d-bbed-a4a16ebca666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table_exists(table_name=DEST_TABLE, engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "024d227d-3dcd-4bd0-8caf-6ab4d2544d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(con=engine, name=DEST_TABLE, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e29ae86a-e9d7-4bbf-a543-5f1a526acc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert chunk #001, size 100,000, took for 5.447 sec.\n",
      "insert chunk #002, size 100,000, took for 5.247 sec.\n",
      "insert chunk #003, size 100,000, took for 5.265 sec.\n",
      "insert chunk #004, size 100,000, took for 5.351 sec.\n",
      "insert chunk #005, size 100,000, took for 5.303 sec.\n",
      "insert chunk #006, size 100,000, took for 5.289 sec.\n",
      "insert chunk #007, size 100,000, took for 5.329 sec.\n",
      "insert chunk #008, size 100,000, took for 5.284 sec.\n",
      "insert chunk #009, size 100,000, took for 5.371 sec.\n",
      "insert chunk #010, size 100,000, took for 5.652 sec.\n",
      "insert chunk #011, size 100,000, took for 5.445 sec.\n",
      "insert chunk #012, size 100,000, took for 5.037 sec.\n",
      "insert chunk #013, size 100,000, took for 5.331 sec.\n",
      "insert chunk #014, size 100,000, took for 5.334 sec.\n",
      "insert chunk #015, size 100,000, took for 5.307 sec.\n",
      "insert chunk #016, size 100,000, took for 5.318 sec.\n",
      "insert chunk #017, size 100,000, took for 5.302 sec.\n",
      "insert chunk #018, size 100,000, took for 5.388 sec.\n",
      "insert chunk #019, size 100,000, took for 5.328 sec.\n",
      "insert chunk #020, size 100,000, took for 5.483 sec.\n",
      "insert chunk #021, size 100,000, took for 5.384 sec.\n",
      "insert chunk #022, size 100,000, took for 5.526 sec.\n",
      "insert chunk #023, size 100,000, took for 5.435 sec.\n",
      "insert chunk #024, size 100,000, took for 5.361 sec.\n",
      "insert chunk #025, size 100,000, took for 5.307 sec.\n",
      "insert chunk #026, size 100,000, took for 5.313 sec.\n",
      "insert chunk #027, size 100,000, took for 5.383 sec.\n",
      "insert chunk #028, size 100,000, took for 5.302 sec.\n",
      "insert chunk #029, size 100,000, took for 5.335 sec.\n",
      "insert chunk #030, size 100,000, took for 5.433 sec.\n",
      "insert chunk #031, size 100,000, took for 5.394 sec.\n",
      "insert chunk #032, size 100,000, took for 5.384 sec.\n",
      "insert chunk #033, size 100,000, took for 5.766 sec.\n",
      "insert chunk #034, size 100,000, took for 5.446 sec.\n",
      "insert chunk #035, size 100,000, took for 5.382 sec.\n",
      "insert chunk #036, size 100,000, took for 5.346 sec.\n",
      "insert chunk #037, size 100,000, took for 5.407 sec.\n",
      "insert chunk #038, size 100,000, took for 5.366 sec.\n",
      "insert chunk #039, size 100,000, took for 5.638 sec.\n",
      "insert chunk #040, size 100,000, took for 5.447 sec.\n",
      "insert chunk #041, size 100,000, took for 5.396 sec.\n",
      "insert chunk #042, size 100,000, took for 5.653 sec.\n",
      "insert chunk #043, size 100,000, took for 5.407 sec.\n",
      "insert chunk #044, size 100,000, took for 5.335 sec.\n",
      "insert chunk #045, size 100,000, took for 5.368 sec.\n",
      "insert chunk #046, size 100,000, took for 5.382 sec.\n",
      "insert chunk #047, size 100,000, took for 5.437 sec.\n",
      "insert chunk #048, size 100,000, took for 5.459 sec.\n",
      "insert chunk #049, size 100,000, took for 5.449 sec.\n",
      "insert chunk #050, size 100,000, took for 5.542 sec.\n",
      "insert chunk #051, size 100,000, took for 5.249 sec.\n",
      "insert chunk #052, size 100,000, took for 5.691 sec.\n",
      "insert chunk #053, size 100,000, took for 5.721 sec.\n",
      "insert chunk #054, size 100,000, took for 5.713 sec.\n",
      "insert chunk #055, size 100,000, took for 5.594 sec.\n",
      "insert chunk #056, size 100,000, took for 5.583 sec.\n",
      "insert chunk #057, size 100,000, took for 5.593 sec.\n",
      "insert chunk #058, size 100,000, took for 5.553 sec.\n",
      "insert chunk #059, size 100,000, took for 5.281 sec.\n",
      "insert chunk #060, size 100,000, took for 5.430 sec.\n",
      "insert chunk #061, size 100,000, took for 5.489 sec.\n",
      "insert chunk #062, size 100,000, took for 5.506 sec.\n",
      "insert chunk #063, size 100,000, took for 5.518 sec.\n",
      "insert chunk #064, size 100,000, took for 5.555 sec.\n",
      "insert chunk #065, size 100,000, took for 5.680 sec.\n",
      "insert chunk #066, size 100,000, took for 5.282 sec.\n",
      "insert chunk #067, size 100,000, took for 5.515 sec.\n",
      "insert chunk #068, size 100,000, took for 5.443 sec.\n",
      "insert chunk #069, size 100,000, took for 5.562 sec.\n",
      "insert chunk #070, size 100,000, took for 5.519 sec.\n",
      "insert chunk #071, size 100,000, took for 5.563 sec.\n",
      "insert chunk #072, size 100,000, took for 5.589 sec.\n",
      "insert chunk #073, size 100,000, took for 5.235 sec.\n",
      "insert chunk #074, size 100,000, took for 5.569 sec.\n",
      "insert chunk #075, size 100,000, took for 5.626 sec.\n",
      "insert chunk #076, size 100,000, took for 5.505 sec.\n",
      "insert chunk #077, size 67,792, took for 4.057 sec.\n"
     ]
    }
   ],
   "source": [
    "df_iter = pd.read_csv(\n",
    "    fn, \n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"], \n",
    "    iterator=True, \n",
    "    chunksize=DATA_CHUNK_SIZE\n",
    ")\n",
    "for i, df_chunk in enumerate(df_iter):\n",
    "    t0 = time()\n",
    "    df_chunk = df_chunk.rename(columns=new_columns_name)\n",
    "    if not db_table_exists(table_name=DEST_TABLE, engine=engine):\n",
    "        df_chunk.head(n=0).to_sql(con=engine, name=DEST_TABLE, if_exists=\"replace\")\n",
    "\n",
    "    df_chunk.to_sql(con=engine, name=DEST_TABLE, if_exists=\"append\")\n",
    "    t1 = time()\n",
    "    print(f\"insert chunk #{i+1:03d}, size {len(df_chunk):,d}, took for {t1-t0:.3f} sec.\")\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d78e840d-fdf7-471e-b62d-f3bdd119b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123,213,213 d \n"
     ]
    }
   ],
   "source": [
    "print(f\"{123213213:,d} d \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
